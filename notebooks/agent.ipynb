{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, Optional\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import json\n",
    "\n",
    "from src.utils.path import from_root\n",
    "from src.yolo.yolo import Yolo\n",
    "from src.llm_caller import LLMCaller\n",
    "from src.llm_detector import Detector\n",
    "from src.edge_validator import EdgeValidator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: /Users/emilstausbol/Documents/GitHub/AIML25-Exam\n"
     ]
    }
   ],
   "source": [
    "# === Ensure working directory is project root ===\n",
    "while Path.cwd().name.lower() != \"aiml25-exam\" and \"aiml25-exam\" in str(Path.cwd()).lower():\n",
    "    os.chdir(\"..\")\n",
    "print(f\"Working directory set to: {Path.cwd()}\")\n",
    "\n",
    "# === Initialize models ===\n",
    "yolo = Yolo(YOLO(from_root(\"models/yolo-trained.pt\")))\n",
    "llm = LLMCaller(\n",
    "    api_key=os.getenv(\"WX_API_KEY\"),\n",
    "    project_id=os.getenv(\"WX_PROJECT_ID\"),\n",
    "    api_url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    model_id=\"watsonx/meta-llama/llama-3-2-90b-vision-instruct\",\n",
    "    params={}\n",
    ")\n",
    "detector = Detector(llm, yolo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === State ===\n",
    "class ImageState(TypedDict):\n",
    "    image_path: str\n",
    "    verbose: bool\n",
    "    is_diagram: Optional[bool]\n",
    "    classification_confidence: Optional[float]\n",
    "    diagram_detections: Optional[list]\n",
    "    cropped_images: Optional[list[str]]\n",
    "    graph: Optional[object]\n",
    "    validation_results: Optional[object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Nodes ===\n",
    "def read_image(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(f\"Reading image: {state['image_path']}\")\n",
    "    return {}\n",
    "\n",
    "def classify_image(state: ImageState):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    image = Image.open(state[\"image_path\"]).convert(\"RGB\")\n",
    "    inputs = processor(text=[\"a diagram\", \"not a diagram\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs.logits_per_image.softmax(dim=1)\n",
    "    is_diagram = probs.argmax().item() == 0\n",
    "    confidence = probs.max().item()\n",
    "    if state[\"verbose\"]:\n",
    "        print(f\"Image classification: {'Diagram' if is_diagram else 'Not a diagram'} (confidence: {confidence:.2f})\")\n",
    "    return {\"is_diagram\": is_diagram, \"classification_confidence\": confidence}\n",
    "\n",
    "def route_decision(state: ImageState) -> str:\n",
    "    return \"run_yolo_detection\" if state[\"is_diagram\"] else \"handle_non_diagram\"\n",
    "\n",
    "def handle_non_diagram(state: ImageState):\n",
    "    print(\"Not a diagram. Ending pipeline.\")\n",
    "    return {}\n",
    "\n",
    "def run_yolo_detection(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"Running YOLO detection...\")\n",
    "    yolo.predict(state[\"image_path\"])\n",
    "    detections = yolo.getBBoxes()\n",
    "\n",
    "    for det in detections:\n",
    "        det[\"coords\"] = det.pop(\"xyxy\")\n",
    "\n",
    "    return {\"diagram_detections\": detections}\n",
    "\n",
    "def crop_diagram(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"✂️ Cropping diagram parts...\")\n",
    "    detections = state.get(\"diagram_detections\", [])\n",
    "    os.makedirs(\"tmp\", exist_ok=True)\n",
    "    image = Image.open(state[\"image_path\"]).convert(\"RGB\")\n",
    "    image_paths = []\n",
    "    for idx, det in enumerate(detections):\n",
    "        x1, y1, x2, y2 = map(int, det[\"coords\"])\n",
    "        cropped = image.crop((x1, y1, x2, y2))\n",
    "        path = f\"tmp/cropped_{idx+1}.png\"\n",
    "        cropped.save(path)\n",
    "        image_paths.append(path)\n",
    "        if state[\"verbose\"]:\n",
    "            print(f\"Saved: {path}\")\n",
    "    return {\"cropped_images\": image_paths}\n",
    "\n",
    "def detect_nodes(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"Detecting nodes...\")\n",
    "    detector.initiate_image(state[\"image_path\"])\n",
    "    detector.detect_nodes()\n",
    "    return {}\n",
    "\n",
    "def detect_edges(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"Detecting edges...\")\n",
    "    detector.detect_edges()\n",
    "    return {}\n",
    "\n",
    "def create_graph(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"Creating graph...\")\n",
    "    return {\"graph\": detector.get_graph()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(ImageState)\n",
    "graph.add_node(\"read_image\", read_image)\n",
    "graph.add_node(\"classify_image\", classify_image)\n",
    "graph.add_node(\"handle_non_diagram\", handle_non_diagram)\n",
    "graph.add_node(\"run_yolo_detection\", run_yolo_detection)\n",
    "graph.add_node(\"crop_diagram\", crop_diagram)\n",
    "graph.add_node(\"detect_nodes\", detect_nodes)\n",
    "graph.add_node(\"detect_edges\", detect_edges)\n",
    "graph.add_node(\"create_graph\", create_graph)\n",
    "\n",
    "graph.add_edge(START, \"read_image\")\n",
    "graph.add_edge(\"read_image\", \"classify_image\")\n",
    "graph.add_conditional_edges(\"classify_image\", route_decision, {\n",
    "    \"run_yolo_detection\": \"run_yolo_detection\",\n",
    "    \"handle_non_diagram\": \"handle_non_diagram\"\n",
    "})\n",
    "graph.add_edge(\"run_yolo_detection\", \"crop_diagram\")\n",
    "graph.add_edge(\"crop_diagram\", \"detect_nodes\")\n",
    "graph.add_edge(\"detect_nodes\", \"detect_edges\")\n",
    "graph.add_edge(\"detect_edges\", \"create_graph\")\n",
    "graph.add_edge(\"create_graph\", END)  \n",
    "graph.add_edge(\"handle_non_diagram\", END)\n",
    "\n",
    "compiled_graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading image: /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/4.png\n",
      "Image classification: Diagram (confidence: 0.64)\n",
      "Running YOLO detection...\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/4.png: 384x640 7 sub-flows, 1049.8ms\n",
      "Speed: 8.7ms preprocess, 1049.8ms inference, 7.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "✂️ Cropping diagram parts...\n",
      "Saved: tmp/cropped_1.png\n",
      "Saved: tmp/cropped_2.png\n",
      "Saved: tmp/cropped_3.png\n",
      "Saved: tmp/cropped_4.png\n",
      "Saved: tmp/cropped_5.png\n",
      "Saved: tmp/cropped_6.png\n",
      "Saved: tmp/cropped_7.png\n",
      "Detecting nodes...\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/4.png: 384x640 7 sub-flows, 915.6ms\n",
      "Speed: 2.5ms preprocess, 915.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "totale images: 7\n",
      "Detecting edges...\n",
      "Creating graph...\n",
      "\n",
      " Final state:\n",
      "- image_path: /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/4.png\n",
      "- verbose: True\n",
      "- is_diagram: True\n",
      "- classification_confidence: 0.6395887732505798\n",
      "- diagram_detections: 7 items\n",
      "- cropped_images: 7 items\n",
      "- graph: <src.graph.Graph object at 0x31d807bc0>\n"
     ]
    }
   ],
   "source": [
    "# === Run ===\n",
    "image_path = str(from_root(\"datasets/test/images/4.png\"))\n",
    "initial_state = {\"image_path\": image_path, \"verbose\": True}\n",
    "result = compiled_graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\n Final state:\")\n",
    "for k, v in result.items():\n",
    "    if isinstance(v, list):\n",
    "        print(f\"- {k}: {len(v)} items\")\n",
    "    else:\n",
    "        print(f\"- {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
