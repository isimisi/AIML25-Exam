{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Optional\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "project_root = Path.cwd()\n",
    "while project_root.name.lower() != \"aiml25-exam\" and \"aiml25-exam\" in str(project_root).lower():\n",
    "    project_root = project_root.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Now you can import from src\n",
    "from src.yolo.detect import run_custom_yolo\n",
    "from src.edge_validator import EdgeValidator\n",
    "from src.llm_detector import Detector\n",
    "from src.llm_caller import LLMCaller\n",
    "from src.yolo.yolo import Yolo\n",
    "from src.graph import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageState(TypedDict):\n",
    "    image_path: str\n",
    "    is_diagram: Optional[bool]\n",
    "    classification_confidence: Optional[float]\n",
    "    diagram_detections: Optional[list]\n",
    "    verbose: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(f\"üîç Reading image: {state['image_path']}\")\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(state: ImageState):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    image = Image.open(state[\"image_path\"]).convert(\"RGB\")\n",
    "    inputs = processor(text=[\"a diagram\", \"not a diagram\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs.logits_per_image.softmax(dim=1)\n",
    "    \n",
    "    is_diagram = probs.argmax() == 0\n",
    "    return {\"is_diagram\": is_diagram, \"classification_confidence\": probs.max().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_non_diagram(state: ImageState):\n",
    "    print(\"üö´ Not a diagram. Ending pipeline.\")\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_decision(state: ImageState) -> str:\n",
    "    return \"run_yolo_detection\" if state[\"is_diagram\"] else \"handle_non_diagram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.yolo.detect import run_custom_yolo  # Import your helper function\n",
    "\n",
    "def run_yolo_detection(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"üì¶ Running custom YOLO model...\")\n",
    "    detections = run_custom_yolo(state[\"image_path\"])\n",
    "    \n",
    "    if state[\"verbose\"]:\n",
    "        for det in detections:\n",
    "            print(f\"Detected: {det['label']} at {det['coords']} (confidence: {det['confidence']:.2f})\")\n",
    "\n",
    "    return {\"diagram_detections\": detections}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def crop_diagram(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"‚úÇÔ∏è Cropping detected parts of the diagram and saving to tmp/ ...\")\n",
    "\n",
    "    cropped_images = []\n",
    "    image = Image.open(state[\"image_path\"]).convert(\"RGB\")\n",
    "\n",
    "    os.makedirs(\"tmp\", exist_ok=True)\n",
    "\n",
    "    for idx, det in enumerate(state.get(\"diagram_detections\", [])):\n",
    "        x1, y1, x2, y2 = det[\"coords\"]\n",
    "\n",
    "        # ‚ö° Convert to integers\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "        cropped = image.crop((x1, y1, x2, y2))\n",
    "        cropped_images.append(cropped)\n",
    "\n",
    "        save_path = f\"tmp/cropped_{idx+1}.png\"\n",
    "        cropped.save(save_path)\n",
    "\n",
    "        if state[\"verbose\"]:\n",
    "            print(f\"üíæ Saved cropped image to {save_path}\")\n",
    "\n",
    "    return {\"cropped_images\": cropped_images}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_nodes(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"üîç Detecting nodes in cropped images...\")\n",
    "    \n",
    "    detector = Detector(model, yolo)\n",
    "    detector.initiate_image(state[\"image_path\"])\n",
    "    detector.detect_nodes()\n",
    "    \n",
    "    return {\"nodes_detected\": True}\n",
    "\n",
    "def detect_edges(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"üîç Detecting edges between nodes...\")\n",
    "    \n",
    "    detector.detect_edges()\n",
    "    return {\"edges_detected\": True}\n",
    "\n",
    "def create_graph(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"üìä Creating graph from detected nodes and edges...\")\n",
    "    \n",
    "    graph = detector.get_graph()\n",
    "    return {\"graph\": graph}\n",
    "\n",
    "def validate_edges(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"‚úÖ Validating detected edges...\")\n",
    "    \n",
    "    validator = EdgeValidator.from_json_file(\n",
    "        str(from_root(\"datasets/test/json/4.json\")), \n",
    "        state[\"graph\"].edges\n",
    "    )\n",
    "    validation_results = validator.validate()\n",
    "    \n",
    "    return {\"validation_results\": validation_results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow definition\n",
    "workflow = StateGraph(ImageState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_edge(START, \"read_image\")\n",
    "workflow.add_node(\"read_image\", read_image)\n",
    "workflow.add_node(\"classify_image\", classify_image)\n",
    "workflow.add_node(\"handle_non_diagram\", handle_non_diagram)\n",
    "workflow.add_node(\"detect_nodes\", detect_nodes)\n",
    "workflow.add_node(\"detect_edges\", detect_edges)\n",
    "workflow.add_node(\"create_graph\", create_graph)\n",
    "workflow.add_node(\"validate_edges\", validate_edges)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"read_image\", \"classify_image\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_image\",\n",
    "    {\n",
    "        \"handle_non_diagram\": lambda x: not x[\"is_diagram\"],\n",
    "        \"detect_nodes\": lambda x: x[\"is_diagram\"]\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"detect_nodes\", \"detect_edges\")\n",
    "workflow.add_edge(\"detect_edges\", \"create_graph\")\n",
    "workflow.add_edge(\"create_graph\", \"validate_edges\")\n",
    "workflow.add_edge(\"validate_edges\", END)\n",
    "workflow.add_edge(\"handle_non_diagram\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "compiled_graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Reading image: ../datasets/test/images/3.png\n"
     ]
    }
   ],
   "source": [
    "# Define once here ‚Äî used everywhere\n",
    "image_path = \"../datasets/test/images/3.png\"\n",
    "\n",
    "# Run the agent\n",
    "result = compiled_graph.invoke({\n",
    "    \"image_path\": image_path,\n",
    "    \"verbose\": True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Visualization Helper of bounding boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def show_image_with_boxes(image_path, detections):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = det[\"coords\"]\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        rect = patches.Rectangle((x1, y1), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1 - 5, f\"{det['label']} ({det['confidence']:.2f})\", color='white', backgroundcolor='red')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå This image was not classified as a diagram. No detection results to show.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Conditionally show results\n",
    "if result.get(\"is_diagram\") and \"diagram_detections\" in result:\n",
    "    show_image_with_boxes(image_path, result[\"diagram_detections\"])\n",
    "else:\n",
    "    print(\"‚ùå This image was not classified as a diagram. No detection results to show.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, img in enumerate(result.get(\"cropped_images\", [])):\n",
    "    print(f\"Cropped part {idx+1}:\")\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
