{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, Optional\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import json\n",
    "\n",
    "from src.utils.path import from_root\n",
    "from src.yolo.yolo import Yolo\n",
    "from src.llm_caller import LLMCaller\n",
    "from src.llm_detector import Detector\n",
    "from src.edge_validator import EdgeValidator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: /Users/emilstausbol/Documents/GitHub/AIML25-Exam\n"
     ]
    }
   ],
   "source": [
    "# === Ensure working directory is project root ===\n",
    "while Path.cwd().name.lower() != \"aiml25-exam\" and \"aiml25-exam\" in str(Path.cwd()).lower():\n",
    "    os.chdir(\"..\")\n",
    "print(f\"Working directory set to: {Path.cwd()}\")\n",
    "\n",
    "# === Initialize models ===\n",
    "yolo = Yolo(YOLO(from_root(\"models/yolo-trained.pt\")))\n",
    "llm = LLMCaller(\n",
    "    api_key=os.getenv(\"WX_API_KEY\"),\n",
    "    project_id=os.getenv(\"WX_PROJECT_ID\"),\n",
    "    api_url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    model_id=\"watsonx/meta-llama/llama-3-2-90b-vision-instruct\",\n",
    "    params={}\n",
    ")\n",
    "detector = Detector(llm, yolo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === State ===\n",
    "class ImageState(TypedDict):\n",
    "    image_path: str\n",
    "    verbose: bool\n",
    "    is_diagram: Optional[bool]\n",
    "    classification_confidence: Optional[float]\n",
    "    diagram_detections: Optional[list]\n",
    "    cropped_images: Optional[list[str]]\n",
    "    graph: Optional[object]\n",
    "    validation_results: Optional[object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Nodes ===\n",
    "def read_image(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(f\"ğŸ” Reading image: {state['image_path']}\")\n",
    "    return {}\n",
    "\n",
    "def classify_image(state: ImageState):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    image = Image.open(state[\"image_path\"]).convert(\"RGB\")\n",
    "    inputs = processor(text=[\"a diagram\", \"not a diagram\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs.logits_per_image.softmax(dim=1)\n",
    "    is_diagram = probs.argmax().item() == 0\n",
    "    confidence = probs.max().item()\n",
    "    if state[\"verbose\"]:\n",
    "        print(f\"ğŸ§  Image classification: {'Diagram' if is_diagram else 'Not a diagram'} (confidence: {confidence:.2f})\")\n",
    "    return {\"is_diagram\": is_diagram, \"classification_confidence\": confidence}\n",
    "\n",
    "def route_decision(state: ImageState) -> str:\n",
    "    return \"run_yolo_detection\" if state[\"is_diagram\"] else \"handle_non_diagram\"\n",
    "\n",
    "def handle_non_diagram(state: ImageState):\n",
    "    print(\"ğŸš« Not a diagram. Ending pipeline.\")\n",
    "    return {}\n",
    "\n",
    "def run_yolo_detection(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"ğŸ“¦ Running YOLO detection...\")\n",
    "    yolo.predict(state[\"image_path\"])\n",
    "    detections = yolo.getBBoxes()\n",
    "\n",
    "    # ğŸ” Fix: normalize keys for downstream steps\n",
    "    for det in detections:\n",
    "        det[\"coords\"] = det.pop(\"xyxy\")\n",
    "\n",
    "    return {\"diagram_detections\": detections}\n",
    "\n",
    "def crop_diagram(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"âœ‚ï¸ Cropping diagram parts...\")\n",
    "    detections = state.get(\"diagram_detections\", [])\n",
    "    os.makedirs(\"tmp\", exist_ok=True)\n",
    "    image = Image.open(state[\"image_path\"]).convert(\"RGB\")\n",
    "    image_paths = []\n",
    "    for idx, det in enumerate(detections):\n",
    "        x1, y1, x2, y2 = map(int, det[\"coords\"])\n",
    "        cropped = image.crop((x1, y1, x2, y2))\n",
    "        path = f\"tmp/cropped_{idx+1}.png\"\n",
    "        cropped.save(path)\n",
    "        image_paths.append(path)\n",
    "        if state[\"verbose\"]:\n",
    "            print(f\"ğŸ’¾ Saved: {path}\")\n",
    "    return {\"cropped_images\": image_paths}\n",
    "\n",
    "def detect_nodes(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"ğŸ” Detecting nodes...\")\n",
    "    detector.initiate_image(state[\"image_path\"])\n",
    "    detector.detect_nodes()\n",
    "    return {}\n",
    "\n",
    "def detect_edges(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"ğŸ”— Detecting edges...\")\n",
    "    detector.detect_edges()\n",
    "    return {}\n",
    "\n",
    "def create_graph(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"ğŸ“Š Creating graph...\")\n",
    "    return {\"graph\": detector.get_graph()}\n",
    "\n",
    "def validate_edges(state: ImageState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"âœ… Validating edges...\")\n",
    "    validator = EdgeValidator.from_json_file(from_root(\"datasets/test/json/4.json\"), detector.get_graph().edges)\n",
    "    return {\"validation_results\": validator.validate()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Graph ===\n",
    "graph = StateGraph(ImageState)\n",
    "graph.add_node(\"read_image\", read_image)\n",
    "graph.add_node(\"classify_image\", classify_image)\n",
    "graph.add_node(\"handle_non_diagram\", handle_non_diagram)\n",
    "graph.add_node(\"run_yolo_detection\", run_yolo_detection)\n",
    "graph.add_node(\"crop_diagram\", crop_diagram)\n",
    "graph.add_node(\"detect_nodes\", detect_nodes)\n",
    "graph.add_node(\"detect_edges\", detect_edges)\n",
    "graph.add_node(\"create_graph\", create_graph)\n",
    "graph.add_node(\"validate_edges\", validate_edges)\n",
    "\n",
    "graph.add_edge(START, \"read_image\")\n",
    "graph.add_edge(\"read_image\", \"classify_image\")\n",
    "graph.add_conditional_edges(\"classify_image\", route_decision, {\n",
    "    \"run_yolo_detection\": \"run_yolo_detection\",\n",
    "    \"handle_non_diagram\": \"handle_non_diagram\"\n",
    "})\n",
    "graph.add_edge(\"run_yolo_detection\", \"crop_diagram\")\n",
    "graph.add_edge(\"crop_diagram\", \"detect_nodes\")\n",
    "graph.add_edge(\"detect_nodes\", \"detect_edges\")\n",
    "graph.add_edge(\"detect_edges\", \"create_graph\")\n",
    "graph.add_edge(\"create_graph\", \"validate_edges\")\n",
    "graph.add_edge(\"validate_edges\", END)\n",
    "graph.add_edge(\"handle_non_diagram\", END)\n",
    "\n",
    "compiled_graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Reading image: /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/4.png\n",
      "ğŸ§  Image classification: Diagram (confidence: 0.64)\n",
      "ğŸ“¦ Running YOLO detection...\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/4.png: 384x640 7 sub-flows, 1010.6ms\n",
      "Speed: 6.1ms preprocess, 1010.6ms inference, 10.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "âœ‚ï¸ Cropping diagram parts...\n",
      "ğŸ’¾ Saved: tmp/cropped_1.png\n",
      "ğŸ’¾ Saved: tmp/cropped_2.png\n",
      "ğŸ’¾ Saved: tmp/cropped_3.png\n",
      "ğŸ’¾ Saved: tmp/cropped_4.png\n",
      "ğŸ’¾ Saved: tmp/cropped_5.png\n",
      "ğŸ’¾ Saved: tmp/cropped_6.png\n",
      "ğŸ’¾ Saved: tmp/cropped_7.png\n",
      "ğŸ” Detecting nodes...\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/4.png: 384x640 7 sub-flows, 885.9ms\n",
      "Speed: 1.9ms preprocess, 885.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "totale images: 7\n",
      "ğŸ”— Detecting edges...\n",
      "ğŸ“Š Creating graph...\n",
      "âœ… Validating edges...\n",
      "\n",
      "ğŸ“¦ Final state:\n",
      "- image_path: /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/4.png\n",
      "- verbose: True\n",
      "- is_diagram: True\n",
      "- classification_confidence: 0.6395887732505798\n",
      "- diagram_detections: 7 items\n",
      "- cropped_images: 7 items\n",
      "- graph: <src.graph.Graph object at 0x31217c110>\n",
      "- validation_results: {'true_positives': ['entity-company 1', 'owner 3-entity', 'fund 2-company 3', 'owner 3-company 3', 'fund 1-company 2'], 'false_positives': ['entity-owner 1', 'company 2-fund 1', 'company 2-fund', 'company 3-fund 2', 'owner-company 3', 'entity-owner', 'entity-company 2'], 'false_negatives': ['owner 1-company 2', 'owner 1-entity'], 'precision': 0.4166666666666667, 'recall': 0.7142857142857143, 'f1_score': 0.5263157894736842}\n"
     ]
    }
   ],
   "source": [
    "# === Run ===\n",
    "image_path = str(from_root(\"datasets/test/images/4.png\"))\n",
    "initial_state = {\"image_path\": image_path, \"verbose\": True}\n",
    "result = compiled_graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\nğŸ“¦ Final state:\")\n",
    "for k, v in result.items():\n",
    "    if isinstance(v, list):\n",
    "        print(f\"- {k}: {len(v)} items\")\n",
    "    else:\n",
    "        print(f\"- {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_with_boxes(image_path, detections):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = det[\"coords\"]\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1 - 5, f\"{det['label']} ({det['confidence']:.2f})\", color='white', backgroundcolor='red')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "if result.get(\"is_diagram\") and \"diagram_detections\" in result:\n",
    "    show_image_with_boxes(image_path, result[\"diagram_detections\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/4.png: 384x640 7 sub-flows, 3893.4ms\n",
      "Speed: 10.1ms preprocess, 3893.4ms inference, 18.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[{'xywh': [1380.986328125, 604.0886840820312, 989.7669677734375, 1017.4681396484375], 'confidence': 0.9350194334983826, 'class_id': 0, 'xyxy': [886.1029052734375, 95.35460662841797, 1875.869873046875, 1112.82275390625]}, {'xywh': [802.1484375, 599.476318359375, 809.3967895507812, 987.23681640625], 'confidence': 0.8156471848487854, 'class_id': 0, 'xyxy': [397.45001220703125, 105.8579330444336, 1206.8468017578125, 1093.0947265625]}, {'xywh': [634.4658813476562, 597.1973266601562, 1238.7115478515625, 993.0460815429688], 'confidence': 0.46774500608444214, 'class_id': 0, 'xyxy': [15.1101655960083, 100.67426300048828, 1253.8216552734375, 1093.7203369140625]}, {'xywh': [444.9051818847656, 602.0018310546875, 868.2406616210938, 1004.81494140625], 'confidence': 0.3852144181728363, 'class_id': 0, 'xyxy': [10.78487491607666, 99.59437561035156, 879.0255126953125, 1104.4093017578125]}, {'xywh': [1736.298828125, 759.9703369140625, 588.7703857421875, 609.4484252929688], 'confidence': 0.37004905939102173, 'class_id': 0, 'xyxy': [1441.9136962890625, 455.24615478515625, 2030.68408203125, 1064.694580078125]}, {'xywh': [893.116943359375, 486.6313171386719, 579.269287109375, 648.6555786132812], 'confidence': 0.36914703249931335, 'class_id': 0, 'xyxy': [603.4822998046875, 162.3035430908203, 1182.7515869140625, 810.9591064453125]}, {'xywh': [644.4774169921875, 636.7406005859375, 424.4251708984375, 865.884521484375], 'confidence': 0.2773652970790863, 'class_id': 0, 'xyxy': [432.26483154296875, 203.79837036132812, 856.6900024414062, 1069.682861328125]}]\n"
     ]
    }
   ],
   "source": [
    "# Optional: Show YOLO detections\n",
    "if result.get(\"is_diagram\") and result.get(\"diagram_detections\"):\n",
    "    def show_image_with_boxes(image_path, detections):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.imshow(image)\n",
    "        for det in detections:\n",
    "            x1, y1, x2, y2 = det[\"coords\"]\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1 - 5, f\"{det['label']} ({det['confidence']:.2f})\", color='white', backgroundcolor='red')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    show_image_with_boxes(result[\"image_path\"], result[\"diagram_detections\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
