{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e6b832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: /Users/emilstausbol/Documents/GitHub/AIML25-Exam\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "while Path.cwd().name.lower() != \"aiml25-exam\" and \"aiml25-exam\" in str(Path.cwd()).lower():\n",
    "    os.chdir(\"..\")  # Move up one directory\n",
    "print(f\"Working directory set to: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169933c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1000 with 0 Axes>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.yolo.yolo import Yolo\n",
    "from ultralytics import YOLO\n",
    "from src.utils.path import from_root\n",
    "from src.llm_caller import LLMCaller\n",
    "from src.llm_detector import Detector\n",
    "from src.mermaid_detector import MermaidDetector\n",
    "from src.mermaid_to_json import MermaidToJSON\n",
    "from src.edge_validator import EdgeValidator\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "plt.figure(figsize=(14, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c64ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(from_root(\"models/yolo-trained.pt\"))\n",
    "\n",
    "yolo = Yolo(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c885b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "\n",
    "# WX_API_KEY = config(\"WX_API_KEY\")\n",
    "# WX_PROJECT_ID = config(\"WX_PROJECT_ID\")\n",
    "# WX_API_URL = \"https://us-south.ml.cloud.ibm.com\"\n",
    "OPENAI_KEY = config(\"OPENAI_API_KEY\")\n",
    "# OPENAI_API_URL = \"https://api.openai.com/v1\"\n",
    "# OPENAI_PROJECT = \"proj_6SmPVkyXa0GChFv7TmDjB9B5\"\n",
    "\n",
    "model = LLMCaller(\n",
    "    api_key=OPENAI_KEY,          \n",
    "    model_id=\"gpt-4o\",     \n",
    "    params={\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 150\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0919fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector(model, yolo)\n",
    "mermaidDetector = MermaidDetector(model, yolo, MermaidToJSON(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55055e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mermaid_with_nodes_crop(path: str, json_path: str):\n",
    "    mermaidDetector.initiate_image(str(from_root(path)))\n",
    "    mermaidDetector.detect_nodes()\n",
    "    mermaidDetector.detect_edges()\n",
    "    mermaidDetector.convert_edges()\n",
    "    graph = mermaidDetector.get_graph()\n",
    "    validator = EdgeValidator.from_json_file(str(from_root(json_path)), graph.edges)\n",
    "    return validator.validate()\n",
    "\n",
    "def mermaid_crop(path: str, json_path: str):\n",
    "    mermaidDetector.initiate_image(str(from_root(path)))\n",
    "    mermaidDetector.detect_edges()\n",
    "    mermaidDetector.convert_edges()\n",
    "    graph = mermaidDetector.get_graph()\n",
    "    validator = EdgeValidator.from_json_file(str(from_root(json_path)), graph.edges)\n",
    "    return validator.validate()\n",
    "\n",
    "def mermaid_no_crop(path: str, json_path: str):\n",
    "    mermaidDetector.initiate_image(str(from_root(path)), should_crop=False)\n",
    "    mermaidDetector.detect_edges()\n",
    "    mermaidDetector.convert_edges()\n",
    "    graph = mermaidDetector.get_graph()\n",
    "    validator = EdgeValidator.from_json_file(str(from_root(json_path)), graph.edges)\n",
    "    return validator.validate()\n",
    "\n",
    "def mermaid_with_nodes_no_crop(path: str, json_path: str):\n",
    "    mermaidDetector.initiate_image(str(from_root(path)), should_crop=False)\n",
    "    mermaidDetector.detect_nodes()\n",
    "    mermaidDetector.detect_edges()\n",
    "    mermaidDetector.convert_edges()\n",
    "    graph = mermaidDetector.get_graph()\n",
    "    validator = EdgeValidator.from_json_file(str(from_root(json_path)), graph.edges)\n",
    "    return validator.validate()\n",
    "\n",
    "def with_nodes_crop(path: str, json_path: str):\n",
    "    detector.initiate_image(str(from_root(path)))\n",
    "    detector.detect_nodes()\n",
    "    detector.detect_edges()\n",
    "    graph = detector.get_graph()\n",
    "    validator = EdgeValidator.from_json_file(str(from_root(json_path)), graph.edges)\n",
    "    return validator.validate()\n",
    "\n",
    "def crop(path: str, json_path: str):\n",
    "    detector.initiate_image(str(from_root(path)))\n",
    "    detector.detect_edges()\n",
    "    graph = detector.get_graph()\n",
    "    validator = EdgeValidator.from_json_file(str(from_root(json_path)), graph.edges)\n",
    "    return validator.validate()\n",
    "\n",
    "def no_crop(path: str, json_path: str):\n",
    "    detector.initiate_image(str(from_root(path)), should_crop=False)\n",
    "    detector.detect_edges()\n",
    "    graph = detector.get_graph()\n",
    "    validator = EdgeValidator.from_json_file(str(from_root(json_path)), graph.edges)\n",
    "    return validator.validate()\n",
    "\n",
    "def with_nodes_no_crop(path: str, json_path: str):\n",
    "    detector.initiate_image(str(from_root(path)), should_crop=False)\n",
    "    detector.detect_nodes()\n",
    "    detector.detect_edges()\n",
    "    graph = detector.get_graph()\n",
    "    validator = EdgeValidator.from_json_file(str(from_root(json_path)), graph.edges)\n",
    "    return validator.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42df233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    [\n",
    "        \"datasets/test/images/1.png\",\n",
    "        \"datasets/test/json/1.json\",\n",
    "    ],\n",
    "    [\n",
    "        \"datasets/test/images/3.png\",\n",
    "        \"datasets/test/json/3.json\",\n",
    "    ],\n",
    "    [\n",
    "        \"datasets/test/images/4.png\",\n",
    "        \"datasets/test/json/4.json\",\n",
    "    ],\n",
    "    [\n",
    "        \"datasets/test/images/5.png\",\n",
    "        \"datasets/test/json/5.json\",\n",
    "    ],\n",
    "    [\n",
    "        \"datasets/test/images/6.png\",\n",
    "        \"datasets/test/json/6.json\",\n",
    "    ],\n",
    "    [\n",
    "        \"datasets/test/images/7.png\",\n",
    "        \"datasets/test/json/7.json\",\n",
    "    ],\n",
    "    [\n",
    "        \"datasets/test/images/8.png\",\n",
    "        \"datasets/test/json/8.json\",\n",
    "    ],\n",
    "    [\n",
    "        \"datasets/test/images/9.png\",\n",
    "        \"datasets/test/json/9.json\",\n",
    "    ],\n",
    "    [\n",
    "        \"datasets/test/images/10.png\",\n",
    "        \"datasets/test/json/10.json\",\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e8441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    \"mermaid_no_crop\": {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": []\n",
    "    },\n",
    "    \"mermaid_crop\": {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": []\n",
    "    },\n",
    "    \"mermaid_with_nodes_no_crop\": {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": []\n",
    "    },\n",
    "    \"mermaid_with_nodes_crop\": {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": []\n",
    "    },\n",
    "    \"no_crop\": {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": []\n",
    "    },\n",
    "    \"crop\": {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": []\n",
    "    },\n",
    "    \"with_nodes_no_crop\": {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": []\n",
    "    },\n",
    "    \"with_nodes_crop\": {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1_score\": []\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6a5517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendScores(val_scores: dict, fn: str):\n",
    "    s = scores[fn]\n",
    "    precision = val_scores[\"precision\"]\n",
    "    s[\"precision\"].append(precision)\n",
    "    f1_score = val_scores[\"f1_score\"]\n",
    "    s[\"f1_score\"].append(f1_score)\n",
    "    recall = val_scores[\"recall\"]\n",
    "    s[\"recall\"].append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678039f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isaac\n",
    "for [img_path, json_path] in dataset:\n",
    "    appendScores(mermaid_crop(img_path, json_path), \"mermaid_crop\")\n",
    "    appendScores(mermaid_with_nodes_crop(img_path, json_path), \"mermaid_with_nodes_crop\")\n",
    "    appendScores(mermaid_no_crop(img_path, json_path), \"mermaid_no_crop\")\n",
    "\n",
    "with open('isaac.json', 'w') as f:\n",
    "    json.dump(scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa88bc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aiml25-exam/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/aiml25-exam/lib/python3.12/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <367D4265-B20F-34BD-94EB-4F3EE47C385B> /opt/anaconda3/envs/aiml25-exam/lib/python3.12/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/aiml25-exam/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/aiml25-exam/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/aiml25-exam/lib/python3.12/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/aiml25-exam/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/1.png: 448x640 14 sub-flows, 1202.6ms\n",
      "Speed: 5.7ms preprocess, 1202.6ms inference, 11.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "totale images: 14\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/1.png: 448x640 14 sub-flows, 1148.5ms\n",
      "Speed: 7.0ms preprocess, 1148.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "totale images: 14\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/3.png: 224x640 8 sub-flows, 639.6ms\n",
      "Speed: 35.1ms preprocess, 639.6ms inference, 13.7ms postprocess per image at shape (1, 3, 224, 640)\n",
      "totale images: 8\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/3.png: 224x640 8 sub-flows, 607.8ms\n",
      "Speed: 2.3ms preprocess, 607.8ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 640)\n",
      "totale images: 8\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/4.png: 384x640 7 sub-flows, 974.8ms\n",
      "Speed: 3.3ms preprocess, 974.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "totale images: 7\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/4.png: 384x640 7 sub-flows, 1015.4ms\n",
      "Speed: 8.5ms preprocess, 1015.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "totale images: 7\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/5.png: 576x640 8 sub-flows, 1404.6ms\n",
      "Speed: 8.1ms preprocess, 1404.6ms inference, 1.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "totale images: 8\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/5.png: 576x640 8 sub-flows, 1867.3ms\n",
      "Speed: 42.7ms preprocess, 1867.3ms inference, 27.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "totale images: 8\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/6.png: 512x640 16 sub-flows, 1313.4ms\n",
      "Speed: 41.7ms preprocess, 1313.4ms inference, 17.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "totale images: 16\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/6.png: 512x640 16 sub-flows, 1418.5ms\n",
      "Speed: 29.6ms preprocess, 1418.5ms inference, 18.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "totale images: 16\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/7.png: 416x640 14 sub-flows, 1116.7ms\n",
      "Speed: 29.2ms preprocess, 1116.7ms inference, 13.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "totale images: 14\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/7.png: 416x640 14 sub-flows, 1040.1ms\n",
      "Speed: 5.4ms preprocess, 1040.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "totale images: 14\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/8.png: 384x640 18 sub-flows, 1027.6ms\n",
      "Speed: 26.5ms preprocess, 1027.6ms inference, 13.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "totale images: 18\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/8.png: 384x640 18 sub-flows, 1018.8ms\n",
      "Speed: 7.0ms preprocess, 1018.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "totale images: 18\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/9.png: 640x640 8 sub-flows, 1588.1ms\n",
      "Speed: 32.2ms preprocess, 1588.1ms inference, 16.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "totale images: 8\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/9.png: 640x640 8 sub-flows, 1559.0ms\n",
      "Speed: 28.9ms preprocess, 1559.0ms inference, 18.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "totale images: 8\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/10.png: 544x640 8 sub-flows, 1378.2ms\n",
      "Speed: 33.2ms preprocess, 1378.2ms inference, 11.9ms postprocess per image at shape (1, 3, 544, 640)\n",
      "totale images: 8\n",
      "\n",
      "image 1/1 /Users/emilstausbol/Documents/GitHub/AIML25-Exam/datasets/test/images/10.png: 544x640 8 sub-flows, 1328.1ms\n",
      "Speed: 5.5ms preprocess, 1328.1ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "totale images: 8\n"
     ]
    }
   ],
   "source": [
    "# Emil\n",
    "for [img_path, json_path] in dataset:\n",
    "    appendScores(mermaid_with_nodes_no_crop(img_path, json_path), \"mermaid_with_nodes_no_crop\")\n",
    "    appendScores(crop(img_path, json_path), \"crop\")\n",
    "    appendScores(with_nodes_crop(img_path, json_path), \"with_nodes_crop\")\n",
    "\n",
    "with open('emil.json', 'w') as f:\n",
    "    json.dump(scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc583966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simon\n",
    "for [img_path, json_path] in dataset:\n",
    "    appendScores(no_crop(img_path, json_path), \"no_crop\")\n",
    "    appendScores(with_nodes_no_crop(img_path, json_path), \"with_nodes_no_crop\")\n",
    "\n",
    "with open('simon.json', 'w') as f:\n",
    "    json.dump(scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96fbe9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
